{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define working directory\n",
    "os.chdir(r\"C:\\Users\\hguzm\\Documents\\000. Personal\\Bootcamp\\Proyectos\\FinalProject-Spotify\\Original_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use glob to match the pattern ‘csv’\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "\n",
    "# Combine all files in the list and export as CSV\n",
    "df = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#export to csv\n",
    "# df.to_csv( \"spotify_data.csv\", index=False, encoding='utf-8-sig')\n",
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "danceability        41106\nenergy              41106\nloudness            41106\nmode                41106\nacousticness        41106\ninstrumentalness    41106\nliveness            41106\nvalence             41106\nduration_ms         41106\ntime_signature      41106\nsections            41106\ntarget              41106\ndtype: int64\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   danceability  energy  loudness  mode  acousticness  instrumentalness  \\\n0         0.578   0.471    -7.270     1        0.3680               0.0   \n1         0.704   0.854    -5.477     0        0.0185               0.0   \n\n   liveness  valence  duration_ms  time_signature  sections  target  \n0     0.159    0.532       196707               4        13       1  \n1     0.148    0.688       242587               4        10       1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>danceability</th>\n      <th>energy</th>\n      <th>loudness</th>\n      <th>mode</th>\n      <th>acousticness</th>\n      <th>instrumentalness</th>\n      <th>liveness</th>\n      <th>valence</th>\n      <th>duration_ms</th>\n      <th>time_signature</th>\n      <th>sections</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.578</td>\n      <td>0.471</td>\n      <td>-7.270</td>\n      <td>1</td>\n      <td>0.3680</td>\n      <td>0.0</td>\n      <td>0.159</td>\n      <td>0.532</td>\n      <td>196707</td>\n      <td>4</td>\n      <td>13</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.704</td>\n      <td>0.854</td>\n      <td>-5.477</td>\n      <td>0</td>\n      <td>0.0185</td>\n      <td>0.0</td>\n      <td>0.148</td>\n      <td>0.688</td>\n      <td>242587</td>\n      <td>4</td>\n      <td>10</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df2 = df.drop(['track', 'artist', 'uri',\"key\", \"tempo\", \"speechiness\", \"chorus_hit\"], axis=1)\n",
    "print(df2.count())\n",
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        danceability  energy  loudness   mode  acousticness  instrumentalness  \\\ntarget                                                                          \n0              20553   20553     20553  20553         20553             20553   \n1              20553   20553     20553  20553         20553             20553   \n\n        liveness  valence  duration_ms  time_signature  sections  \ntarget                                                            \n0          20553    20553        20553           20553     20553  \n1          20553    20553        20553           20553     20553  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>danceability</th>\n      <th>energy</th>\n      <th>loudness</th>\n      <th>mode</th>\n      <th>acousticness</th>\n      <th>instrumentalness</th>\n      <th>liveness</th>\n      <th>valence</th>\n      <th>duration_ms</th>\n      <th>time_signature</th>\n      <th>sections</th>\n    </tr>\n    <tr>\n      <th>target</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>20553</td>\n      <td>20553</td>\n      <td>20553</td>\n      <td>20553</td>\n      <td>20553</td>\n      <td>20553</td>\n      <td>20553</td>\n      <td>20553</td>\n      <td>20553</td>\n      <td>20553</td>\n      <td>20553</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>20553</td>\n      <td>20553</td>\n      <td>20553</td>\n      <td>20553</td>\n      <td>20553</td>\n      <td>20553</td>\n      <td>20553</td>\n      <td>20553</td>\n      <td>20553</td>\n      <td>20553</td>\n      <td>20553</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "#Validation of distribution of output values\n",
    "df2.groupby(\"target\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                  danceability    energy  loudness      mode  acousticness  \\\ndanceability          1.000000  0.206128  0.274160 -0.032743     -0.261252   \nenergy                0.206128  1.000000  0.772628 -0.033780     -0.715088   \nloudness              0.274160  0.772628  1.000000  0.000509     -0.566548   \nmode                 -0.032743 -0.033780  0.000509  1.000000      0.049961   \nacousticness         -0.261252 -0.715088 -0.566548  0.049961      1.000000   \ninstrumentalness     -0.301915 -0.208153 -0.374301 -0.075992      0.204137   \nliveness             -0.115251  0.157779  0.086708  0.008886     -0.030765   \nvalence               0.553858  0.341427  0.271775  0.035636     -0.195331   \nduration_ms          -0.063030  0.011830 -0.049890 -0.074828     -0.070271   \ntime_signature        0.191664  0.196072  0.165520 -0.009483     -0.188139   \nsections             -0.058615 -0.040233 -0.080065 -0.057095     -0.016824   \ntarget                0.346097  0.177142  0.286034  0.079614     -0.246036   \n\n                  instrumentalness  liveness   valence  duration_ms  \\\ndanceability             -0.301915 -0.115251  0.553858    -0.063030   \nenergy                   -0.208153  0.157779  0.341427     0.011830   \nloudness                 -0.374301  0.086708  0.271775    -0.049890   \nmode                     -0.075992  0.008886  0.035636    -0.074828   \nacousticness              0.204137 -0.030765 -0.195331    -0.070271   \ninstrumentalness          1.000000 -0.050582 -0.287047     0.106551   \nliveness                 -0.050582  1.000000  0.000784     0.005336   \nvalence                  -0.287047  0.000784  1.000000    -0.172166   \nduration_ms               0.106551  0.005336 -0.172166     1.000000   \ntime_signature           -0.077980  0.004773  0.155669     0.018454   \nsections                  0.081188 -0.011742 -0.133527     0.888952   \ntarget                   -0.407638 -0.051445  0.251147    -0.073820   \n\n                  time_signature  sections    target  \ndanceability            0.191664 -0.058615  0.346097  \nenergy                  0.196072 -0.040233  0.177142  \nloudness                0.165520 -0.080065  0.286034  \nmode                   -0.009483 -0.057095  0.079614  \nacousticness           -0.188139 -0.016824 -0.246036  \ninstrumentalness       -0.077980  0.081188 -0.407638  \nliveness                0.004773 -0.011742 -0.051445  \nvalence                 0.155669 -0.133527  0.251147  \nduration_ms             0.018454  0.888952 -0.073820  \ntime_signature          1.000000  0.008424  0.104884  \nsections                0.008424  1.000000 -0.059997  \ntarget                  0.104884 -0.059997  1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>danceability</th>\n      <th>energy</th>\n      <th>loudness</th>\n      <th>mode</th>\n      <th>acousticness</th>\n      <th>instrumentalness</th>\n      <th>liveness</th>\n      <th>valence</th>\n      <th>duration_ms</th>\n      <th>time_signature</th>\n      <th>sections</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>danceability</td>\n      <td>1.000000</td>\n      <td>0.206128</td>\n      <td>0.274160</td>\n      <td>-0.032743</td>\n      <td>-0.261252</td>\n      <td>-0.301915</td>\n      <td>-0.115251</td>\n      <td>0.553858</td>\n      <td>-0.063030</td>\n      <td>0.191664</td>\n      <td>-0.058615</td>\n      <td>0.346097</td>\n    </tr>\n    <tr>\n      <td>energy</td>\n      <td>0.206128</td>\n      <td>1.000000</td>\n      <td>0.772628</td>\n      <td>-0.033780</td>\n      <td>-0.715088</td>\n      <td>-0.208153</td>\n      <td>0.157779</td>\n      <td>0.341427</td>\n      <td>0.011830</td>\n      <td>0.196072</td>\n      <td>-0.040233</td>\n      <td>0.177142</td>\n    </tr>\n    <tr>\n      <td>loudness</td>\n      <td>0.274160</td>\n      <td>0.772628</td>\n      <td>1.000000</td>\n      <td>0.000509</td>\n      <td>-0.566548</td>\n      <td>-0.374301</td>\n      <td>0.086708</td>\n      <td>0.271775</td>\n      <td>-0.049890</td>\n      <td>0.165520</td>\n      <td>-0.080065</td>\n      <td>0.286034</td>\n    </tr>\n    <tr>\n      <td>mode</td>\n      <td>-0.032743</td>\n      <td>-0.033780</td>\n      <td>0.000509</td>\n      <td>1.000000</td>\n      <td>0.049961</td>\n      <td>-0.075992</td>\n      <td>0.008886</td>\n      <td>0.035636</td>\n      <td>-0.074828</td>\n      <td>-0.009483</td>\n      <td>-0.057095</td>\n      <td>0.079614</td>\n    </tr>\n    <tr>\n      <td>acousticness</td>\n      <td>-0.261252</td>\n      <td>-0.715088</td>\n      <td>-0.566548</td>\n      <td>0.049961</td>\n      <td>1.000000</td>\n      <td>0.204137</td>\n      <td>-0.030765</td>\n      <td>-0.195331</td>\n      <td>-0.070271</td>\n      <td>-0.188139</td>\n      <td>-0.016824</td>\n      <td>-0.246036</td>\n    </tr>\n    <tr>\n      <td>instrumentalness</td>\n      <td>-0.301915</td>\n      <td>-0.208153</td>\n      <td>-0.374301</td>\n      <td>-0.075992</td>\n      <td>0.204137</td>\n      <td>1.000000</td>\n      <td>-0.050582</td>\n      <td>-0.287047</td>\n      <td>0.106551</td>\n      <td>-0.077980</td>\n      <td>0.081188</td>\n      <td>-0.407638</td>\n    </tr>\n    <tr>\n      <td>liveness</td>\n      <td>-0.115251</td>\n      <td>0.157779</td>\n      <td>0.086708</td>\n      <td>0.008886</td>\n      <td>-0.030765</td>\n      <td>-0.050582</td>\n      <td>1.000000</td>\n      <td>0.000784</td>\n      <td>0.005336</td>\n      <td>0.004773</td>\n      <td>-0.011742</td>\n      <td>-0.051445</td>\n    </tr>\n    <tr>\n      <td>valence</td>\n      <td>0.553858</td>\n      <td>0.341427</td>\n      <td>0.271775</td>\n      <td>0.035636</td>\n      <td>-0.195331</td>\n      <td>-0.287047</td>\n      <td>0.000784</td>\n      <td>1.000000</td>\n      <td>-0.172166</td>\n      <td>0.155669</td>\n      <td>-0.133527</td>\n      <td>0.251147</td>\n    </tr>\n    <tr>\n      <td>duration_ms</td>\n      <td>-0.063030</td>\n      <td>0.011830</td>\n      <td>-0.049890</td>\n      <td>-0.074828</td>\n      <td>-0.070271</td>\n      <td>0.106551</td>\n      <td>0.005336</td>\n      <td>-0.172166</td>\n      <td>1.000000</td>\n      <td>0.018454</td>\n      <td>0.888952</td>\n      <td>-0.073820</td>\n    </tr>\n    <tr>\n      <td>time_signature</td>\n      <td>0.191664</td>\n      <td>0.196072</td>\n      <td>0.165520</td>\n      <td>-0.009483</td>\n      <td>-0.188139</td>\n      <td>-0.077980</td>\n      <td>0.004773</td>\n      <td>0.155669</td>\n      <td>0.018454</td>\n      <td>1.000000</td>\n      <td>0.008424</td>\n      <td>0.104884</td>\n    </tr>\n    <tr>\n      <td>sections</td>\n      <td>-0.058615</td>\n      <td>-0.040233</td>\n      <td>-0.080065</td>\n      <td>-0.057095</td>\n      <td>-0.016824</td>\n      <td>0.081188</td>\n      <td>-0.011742</td>\n      <td>-0.133527</td>\n      <td>0.888952</td>\n      <td>0.008424</td>\n      <td>1.000000</td>\n      <td>-0.059997</td>\n    </tr>\n    <tr>\n      <td>target</td>\n      <td>0.346097</td>\n      <td>0.177142</td>\n      <td>0.286034</td>\n      <td>0.079614</td>\n      <td>-0.246036</td>\n      <td>-0.407638</td>\n      <td>-0.051445</td>\n      <td>0.251147</td>\n      <td>-0.073820</td>\n      <td>0.104884</td>\n      <td>-0.059997</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "#Validation of correlation between variables\n",
    "import numpy as np \n",
    "df3=df2.corr()\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(41106, 12) (41106, 11) (41106,)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[  0.578,   0.471,  -7.27 , ...,   4.   ,  13.   ,   1.   ],\n       [  0.704,   0.854,  -5.477, ...,   4.   ,  10.   ,   1.   ],\n       [  0.162,   0.836,  -3.009, ...,   4.   ,  13.   ,   0.   ],\n       ...,\n       [  0.562,   0.314, -15.213, ...,   4.   ,  10.   ,   1.   ],\n       [  0.622,   0.781,  -6.08 , ...,   4.   ,  11.   ,   0.   ],\n       [  0.664,   0.739,  -9.005, ...,   4.   ,  14.   ,   1.   ]])"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Reformat data\n",
    "data = df2.values\n",
    "X = data[:, 0:11]  \n",
    "y = data[:, 11]\n",
    "\n",
    "print(data.shape, X.shape, y.shape)\n",
    "data\n",
    "# X[0]\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.78947368, 0.52283395, 0.76172236, ..., 0.05569473, 0.8       ,\n        0.06508876],\n       [0.54048583, 0.83094117, 0.77921392, ..., 0.03957652, 0.8       ,\n        0.04142012],\n       [0.5       , 0.66588373, 0.7455139 , ..., 0.03176775, 0.8       ,\n        0.03550296],\n       ...,\n       [0.57692308, 0.36777999, 0.68303866, ..., 0.09203629, 0.8       ,\n        0.09467456],\n       [0.41902834, 0.16070793, 0.50819858, ..., 0.0441886 , 0.8       ,\n        0.04733728],\n       [0.29554656, 0.24873856, 0.67937808, ..., 0.0506431 , 0.6       ,\n        0.06508876]])"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "#Scale features (X) using MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_scaler = MinMaxScaler(feature_range=(0,1)).fit(X_train)\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test) \n",
    "\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 1.],\n       [1., 0.],\n       [0., 1.],\n       ...,\n       [0., 1.],\n       [0., 1.],\n       [1., 0.]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "#One-hot encode output labels (y)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)\n",
    "\n",
    "y_train_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and defining our Deep Learning Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a sequential model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "model = Sequential() \n",
    "\n",
    "\n",
    "number_inputs = 11  \n",
    "\n",
    "#Create hidden layers\n",
    "model.add(Dense(units=14,activation='relu', input_dim=number_inputs))\n",
    "model.add(Dense(units=120,activation='relu'))\n",
    "model.add(Dense(units=80,activation='relu'))\n",
    "\n",
    "#Create output layer\n",
    "number_classes = 2\n",
    "model.add(Dense(units=number_classes, activation='softmax')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Hidden Nodes \n",
    "https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw#:~:text=The%20number%20of%20hidden%20neurons,size%20of%20the%20input%20layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 14)                168       \n_________________________________________________________________\ndense_1 (Dense)              (None, 120)               1800      \n_________________________________________________________________\ndense_2 (Dense)              (None, 80)                9680      \n_________________________________________________________________\ndense_3 (Dense)              (None, 2)                 162       \n=================================================================\nTotal params: 11,810\nTrainable params: 11,810\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "#Model Summary\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the Model\n",
    "import tensorflow as tf\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "poch 162/500\n16/16 - 0s - loss: 0.4747 - accuracy: 0.7677\nEpoch 163/500\n16/16 - 0s - loss: 0.4752 - accuracy: 0.7663\nEpoch 164/500\n16/16 - 0s - loss: 0.4758 - accuracy: 0.7667\nEpoch 165/500\n16/16 - 0s - loss: 0.4770 - accuracy: 0.7648\nEpoch 166/500\n16/16 - 0s - loss: 0.4740 - accuracy: 0.7675\nEpoch 167/500\n16/16 - 0s - loss: 0.4741 - accuracy: 0.7689\nEpoch 168/500\n16/16 - 0s - loss: 0.4741 - accuracy: 0.7684\nEpoch 169/500\n16/16 - 0s - loss: 0.4738 - accuracy: 0.7683\nEpoch 170/500\n16/16 - 0s - loss: 0.4740 - accuracy: 0.7685\nEpoch 171/500\n16/16 - 0s - loss: 0.4749 - accuracy: 0.7684\nEpoch 172/500\n16/16 - 0s - loss: 0.4745 - accuracy: 0.7684\nEpoch 173/500\n16/16 - 0s - loss: 0.4739 - accuracy: 0.7674\nEpoch 174/500\n16/16 - 0s - loss: 0.4730 - accuracy: 0.7689\nEpoch 175/500\n16/16 - 0s - loss: 0.4729 - accuracy: 0.7686\nEpoch 176/500\n16/16 - 0s - loss: 0.4744 - accuracy: 0.7689\nEpoch 177/500\n16/16 - 0s - loss: 0.4739 - accuracy: 0.7685\nEpoch 178/500\n16/16 - 0s - loss: 0.4729 - accuracy: 0.7693\nEpoch 179/500\n16/16 - 0s - loss: 0.4738 - accuracy: 0.7671\nEpoch 180/500\n16/16 - 0s - loss: 0.4736 - accuracy: 0.7680\nEpoch 181/500\n16/16 - 0s - loss: 0.4738 - accuracy: 0.7674\nEpoch 182/500\n16/16 - 0s - loss: 0.4724 - accuracy: 0.7688\nEpoch 183/500\n16/16 - 0s - loss: 0.4736 - accuracy: 0.7677\nEpoch 184/500\n16/16 - 0s - loss: 0.4722 - accuracy: 0.7695\nEpoch 185/500\n16/16 - 0s - loss: 0.4732 - accuracy: 0.7697\nEpoch 186/500\n16/16 - 0s - loss: 0.4732 - accuracy: 0.7681\nEpoch 187/500\n16/16 - 0s - loss: 0.4721 - accuracy: 0.7699\nEpoch 188/500\n16/16 - 0s - loss: 0.4716 - accuracy: 0.7685\nEpoch 189/500\n16/16 - 0s - loss: 0.4718 - accuracy: 0.7697\nEpoch 190/500\n16/16 - 0s - loss: 0.4713 - accuracy: 0.7703\nEpoch 191/500\n16/16 - 0s - loss: 0.4721 - accuracy: 0.7696\nEpoch 192/500\n16/16 - 0s - loss: 0.4716 - accuracy: 0.7704\nEpoch 193/500\n16/16 - 0s - loss: 0.4716 - accuracy: 0.7695\nEpoch 194/500\n16/16 - 0s - loss: 0.4720 - accuracy: 0.7706\nEpoch 195/500\n16/16 - 0s - loss: 0.4726 - accuracy: 0.7702\nEpoch 196/500\n16/16 - 0s - loss: 0.4714 - accuracy: 0.7692\nEpoch 197/500\n16/16 - 0s - loss: 0.4718 - accuracy: 0.7690\nEpoch 198/500\n16/16 - 0s - loss: 0.4733 - accuracy: 0.7698\nEpoch 199/500\n16/16 - 0s - loss: 0.4714 - accuracy: 0.7699\nEpoch 200/500\n16/16 - 0s - loss: 0.4715 - accuracy: 0.7700\nEpoch 201/500\n16/16 - 0s - loss: 0.4731 - accuracy: 0.7696\nEpoch 202/500\n16/16 - 0s - loss: 0.4717 - accuracy: 0.7696\nEpoch 203/500\n16/16 - 0s - loss: 0.4703 - accuracy: 0.7706\nEpoch 204/500\n16/16 - 0s - loss: 0.4704 - accuracy: 0.7711\nEpoch 205/500\n16/16 - 0s - loss: 0.4703 - accuracy: 0.7713\nEpoch 206/500\n16/16 - 0s - loss: 0.4703 - accuracy: 0.7711\nEpoch 207/500\n16/16 - 0s - loss: 0.4707 - accuracy: 0.7699\nEpoch 208/500\n16/16 - 0s - loss: 0.4708 - accuracy: 0.7708\nEpoch 209/500\n16/16 - 0s - loss: 0.4720 - accuracy: 0.7706\nEpoch 210/500\n16/16 - 0s - loss: 0.4711 - accuracy: 0.7697\nEpoch 211/500\n16/16 - 0s - loss: 0.4702 - accuracy: 0.7706\nEpoch 212/500\n16/16 - 0s - loss: 0.4690 - accuracy: 0.7708\nEpoch 213/500\n16/16 - 0s - loss: 0.4700 - accuracy: 0.7710\nEpoch 214/500\n16/16 - 0s - loss: 0.4694 - accuracy: 0.7714\nEpoch 215/500\n16/16 - 0s - loss: 0.4694 - accuracy: 0.7714\nEpoch 216/500\n16/16 - 0s - loss: 0.4696 - accuracy: 0.7708\nEpoch 217/500\n16/16 - 0s - loss: 0.4699 - accuracy: 0.7706\nEpoch 218/500\n16/16 - 0s - loss: 0.4701 - accuracy: 0.7708\nEpoch 219/500\n16/16 - 0s - loss: 0.4696 - accuracy: 0.7701\nEpoch 220/500\n16/16 - 0s - loss: 0.4700 - accuracy: 0.7711\nEpoch 221/500\n16/16 - 0s - loss: 0.4707 - accuracy: 0.7696\nEpoch 222/500\n16/16 - 0s - loss: 0.4703 - accuracy: 0.7696\nEpoch 223/500\n16/16 - 0s - loss: 0.4698 - accuracy: 0.7719\nEpoch 224/500\n16/16 - 0s - loss: 0.4693 - accuracy: 0.7713\nEpoch 225/500\n16/16 - 0s - loss: 0.4683 - accuracy: 0.7708\nEpoch 226/500\n16/16 - 0s - loss: 0.4683 - accuracy: 0.7731\nEpoch 227/500\n16/16 - 0s - loss: 0.4675 - accuracy: 0.7726\nEpoch 228/500\n16/16 - 0s - loss: 0.4687 - accuracy: 0.7713\nEpoch 229/500\n16/16 - 0s - loss: 0.4697 - accuracy: 0.7702\nEpoch 230/500\n16/16 - 0s - loss: 0.4687 - accuracy: 0.7712\nEpoch 231/500\n16/16 - 0s - loss: 0.4686 - accuracy: 0.7721\nEpoch 232/500\n16/16 - 0s - loss: 0.4693 - accuracy: 0.7706\nEpoch 233/500\n16/16 - 0s - loss: 0.4685 - accuracy: 0.7720\nEpoch 234/500\n16/16 - 0s - loss: 0.4695 - accuracy: 0.7713\nEpoch 235/500\n16/16 - 0s - loss: 0.4683 - accuracy: 0.7718\nEpoch 236/500\n16/16 - 0s - loss: 0.4696 - accuracy: 0.7712\nEpoch 237/500\n16/16 - 0s - loss: 0.4680 - accuracy: 0.7709\nEpoch 238/500\n16/16 - 0s - loss: 0.4673 - accuracy: 0.7729\nEpoch 239/500\n16/16 - 0s - loss: 0.4671 - accuracy: 0.7726\nEpoch 240/500\n16/16 - 0s - loss: 0.4680 - accuracy: 0.7708\nEpoch 241/500\n16/16 - 0s - loss: 0.4679 - accuracy: 0.7720\nEpoch 242/500\n16/16 - 0s - loss: 0.4679 - accuracy: 0.7723\nEpoch 243/500\n16/16 - 0s - loss: 0.4683 - accuracy: 0.7726\nEpoch 244/500\n16/16 - 0s - loss: 0.4680 - accuracy: 0.7717\nEpoch 245/500\n16/16 - 0s - loss: 0.4659 - accuracy: 0.7739\nEpoch 246/500\n16/16 - 0s - loss: 0.4668 - accuracy: 0.7730\nEpoch 247/500\n16/16 - 0s - loss: 0.4671 - accuracy: 0.7723\nEpoch 248/500\n16/16 - 0s - loss: 0.4662 - accuracy: 0.7740\nEpoch 249/500\n16/16 - 0s - loss: 0.4664 - accuracy: 0.7726\nEpoch 250/500\n16/16 - 0s - loss: 0.4665 - accuracy: 0.7729\nEpoch 251/500\n16/16 - 0s - loss: 0.4662 - accuracy: 0.7729\nEpoch 252/500\n16/16 - 0s - loss: 0.4658 - accuracy: 0.7737\nEpoch 253/500\n16/16 - 0s - loss: 0.4667 - accuracy: 0.7739\nEpoch 254/500\n16/16 - 0s - loss: 0.4660 - accuracy: 0.7739\nEpoch 255/500\n16/16 - 0s - loss: 0.4654 - accuracy: 0.7737\nEpoch 256/500\n16/16 - 0s - loss: 0.4667 - accuracy: 0.7705\nEpoch 257/500\n16/16 - 0s - loss: 0.4658 - accuracy: 0.7732\nEpoch 258/500\n16/16 - 0s - loss: 0.4649 - accuracy: 0.7741\nEpoch 259/500\n16/16 - 0s - loss: 0.4654 - accuracy: 0.7730\nEpoch 260/500\n16/16 - 0s - loss: 0.4644 - accuracy: 0.7743\nEpoch 261/500\n16/16 - 0s - loss: 0.4645 - accuracy: 0.7731\nEpoch 262/500\n16/16 - 0s - loss: 0.4640 - accuracy: 0.7734\nEpoch 263/500\n16/16 - 0s - loss: 0.4655 - accuracy: 0.7749\nEpoch 264/500\n16/16 - 0s - loss: 0.4655 - accuracy: 0.7752\nEpoch 265/500\n16/16 - 0s - loss: 0.4644 - accuracy: 0.7749\nEpoch 266/500\n16/16 - 0s - loss: 0.4647 - accuracy: 0.7745\nEpoch 267/500\n16/16 - 0s - loss: 0.4634 - accuracy: 0.7751\nEpoch 268/500\n16/16 - 0s - loss: 0.4644 - accuracy: 0.7742\nEpoch 269/500\n16/16 - 0s - loss: 0.4641 - accuracy: 0.7746\nEpoch 270/500\n16/16 - 0s - loss: 0.4648 - accuracy: 0.7733\nEpoch 271/500\n16/16 - 0s - loss: 0.4641 - accuracy: 0.7731\nEpoch 272/500\n16/16 - 0s - loss: 0.4638 - accuracy: 0.7751\nEpoch 273/500\n16/16 - 0s - loss: 0.4637 - accuracy: 0.7748\nEpoch 274/500\n16/16 - 0s - loss: 0.4644 - accuracy: 0.7739\nEpoch 275/500\n16/16 - 0s - loss: 0.4646 - accuracy: 0.7747\nEpoch 276/500\n16/16 - 0s - loss: 0.4639 - accuracy: 0.7738\nEpoch 277/500\n16/16 - 0s - loss: 0.4644 - accuracy: 0.7747\nEpoch 278/500\n16/16 - 0s - loss: 0.4632 - accuracy: 0.7748\nEpoch 279/500\n16/16 - 0s - loss: 0.4639 - accuracy: 0.7756\nEpoch 280/500\n16/16 - 0s - loss: 0.4627 - accuracy: 0.7743\nEpoch 281/500\n16/16 - 0s - loss: 0.4626 - accuracy: 0.7763\nEpoch 282/500\n16/16 - 0s - loss: 0.4626 - accuracy: 0.7762\nEpoch 283/500\n16/16 - 0s - loss: 0.4626 - accuracy: 0.7754\nEpoch 284/500\n16/16 - 0s - loss: 0.4638 - accuracy: 0.7727\nEpoch 285/500\n16/16 - 0s - loss: 0.4633 - accuracy: 0.7755\nEpoch 286/500\n16/16 - 0s - loss: 0.4632 - accuracy: 0.7763\nEpoch 287/500\n16/16 - 0s - loss: 0.4624 - accuracy: 0.7751\nEpoch 288/500\n16/16 - 0s - loss: 0.4618 - accuracy: 0.7758\nEpoch 289/500\n16/16 - 0s - loss: 0.4625 - accuracy: 0.7759\nEpoch 290/500\n16/16 - 0s - loss: 0.4628 - accuracy: 0.7751\nEpoch 291/500\n16/16 - 0s - loss: 0.4617 - accuracy: 0.7772\nEpoch 292/500\n16/16 - 0s - loss: 0.4630 - accuracy: 0.7752\nEpoch 293/500\n16/16 - 0s - loss: 0.4634 - accuracy: 0.7765\nEpoch 294/500\n16/16 - 0s - loss: 0.4615 - accuracy: 0.7760\nEpoch 295/500\n16/16 - 0s - loss: 0.4610 - accuracy: 0.7759\nEpoch 296/500\n16/16 - 0s - loss: 0.4608 - accuracy: 0.7770\nEpoch 297/500\n16/16 - 0s - loss: 0.4608 - accuracy: 0.7761\nEpoch 298/500\n16/16 - 0s - loss: 0.4613 - accuracy: 0.7771\nEpoch 299/500\n16/16 - 0s - loss: 0.4610 - accuracy: 0.7770\nEpoch 300/500\n16/16 - 0s - loss: 0.4613 - accuracy: 0.7772\nEpoch 301/500\n16/16 - 0s - loss: 0.4609 - accuracy: 0.7751\nEpoch 302/500\n16/16 - 0s - loss: 0.4615 - accuracy: 0.7764\nEpoch 303/500\n16/16 - 0s - loss: 0.4647 - accuracy: 0.7737\nEpoch 304/500\n16/16 - 0s - loss: 0.4639 - accuracy: 0.7751\nEpoch 305/500\n16/16 - 0s - loss: 0.4612 - accuracy: 0.7771\nEpoch 306/500\n16/16 - 0s - loss: 0.4604 - accuracy: 0.7773\nEpoch 307/500\n16/16 - 0s - loss: 0.4610 - accuracy: 0.7754\nEpoch 308/500\n16/16 - 0s - loss: 0.4604 - accuracy: 0.7771\nEpoch 309/500\n16/16 - 0s - loss: 0.4616 - accuracy: 0.7758\nEpoch 310/500\n16/16 - 0s - loss: 0.4606 - accuracy: 0.7767\nEpoch 311/500\n16/16 - 0s - loss: 0.4625 - accuracy: 0.7761\nEpoch 312/500\n16/16 - 0s - loss: 0.4613 - accuracy: 0.7755\nEpoch 313/500\n16/16 - 0s - loss: 0.4615 - accuracy: 0.7768\nEpoch 314/500\n16/16 - 0s - loss: 0.4608 - accuracy: 0.7769\nEpoch 315/500\n16/16 - 0s - loss: 0.4598 - accuracy: 0.7775\nEpoch 316/500\n16/16 - 0s - loss: 0.4599 - accuracy: 0.7768\nEpoch 317/500\n16/16 - 0s - loss: 0.4600 - accuracy: 0.7763\nEpoch 318/500\n16/16 - 0s - loss: 0.4598 - accuracy: 0.7760\nEpoch 319/500\n16/16 - 0s - loss: 0.4600 - accuracy: 0.7774\nEpoch 320/500\n16/16 - 0s - loss: 0.4600 - accuracy: 0.7776\nEpoch 321/500\n16/16 - 0s - loss: 0.4618 - accuracy: 0.7764\nEpoch 322/500\n16/16 - 0s - loss: 0.4611 - accuracy: 0.7755\nEpoch 323/500\n16/16 - 0s - loss: 0.4593 - accuracy: 0.7775\nEpoch 324/500\n16/16 - 0s - loss: 0.4610 - accuracy: 0.7758\nEpoch 325/500\n16/16 - 0s - loss: 0.4588 - accuracy: 0.7783\nEpoch 326/500\n16/16 - 0s - loss: 0.4589 - accuracy: 0.7765\nEpoch 327/500\n16/16 - 0s - loss: 0.4597 - accuracy: 0.7762\nEpoch 328/500\n16/16 - 0s - loss: 0.4617 - accuracy: 0.7760\nEpoch 329/500\n16/16 - 0s - loss: 0.4593 - accuracy: 0.7776\nEpoch 330/500\n16/16 - 0s - loss: 0.4596 - accuracy: 0.7768\nEpoch 331/500\n16/16 - 0s - loss: 0.4596 - accuracy: 0.7772\nEpoch 332/500\n16/16 - 0s - loss: 0.4577 - accuracy: 0.7790\nEpoch 333/500\n16/16 - 0s - loss: 0.4582 - accuracy: 0.7788\nEpoch 334/500\n16/16 - 0s - loss: 0.4589 - accuracy: 0.7782\nEpoch 335/500\n16/16 - 0s - loss: 0.4605 - accuracy: 0.7772\nEpoch 336/500\n16/16 - 0s - loss: 0.4584 - accuracy: 0.7782\nEpoch 337/500\n16/16 - 0s - loss: 0.4579 - accuracy: 0.7779\nEpoch 338/500\n16/16 - 0s - loss: 0.4581 - accuracy: 0.7793\nEpoch 339/500\n16/16 - 0s - loss: 0.4625 - accuracy: 0.7761\nEpoch 340/500\n16/16 - 0s - loss: 0.4617 - accuracy: 0.7757\nEpoch 341/500\n16/16 - 0s - loss: 0.4586 - accuracy: 0.7782\nEpoch 342/500\n16/16 - 0s - loss: 0.4588 - accuracy: 0.7775\nEpoch 343/500\n16/16 - 0s - loss: 0.4592 - accuracy: 0.7774\nEpoch 344/500\n16/16 - 0s - loss: 0.4574 - accuracy: 0.7793\nEpoch 345/500\n16/16 - 0s - loss: 0.4570 - accuracy: 0.7778\nEpoch 346/500\n16/16 - 0s - loss: 0.4592 - accuracy: 0.7773\nEpoch 347/500\n16/16 - 0s - loss: 0.4573 - accuracy: 0.7799\nEpoch 348/500\n16/16 - 0s - loss: 0.4571 - accuracy: 0.7794\nEpoch 349/500\n16/16 - 0s - loss: 0.4566 - accuracy: 0.7790\nEpoch 350/500\n16/16 - 0s - loss: 0.4564 - accuracy: 0.7789\nEpoch 351/500\n16/16 - 0s - loss: 0.4566 - accuracy: 0.7786\nEpoch 352/500\n16/16 - 0s - loss: 0.4567 - accuracy: 0.7792\nEpoch 353/500\n16/16 - 0s - loss: 0.4567 - accuracy: 0.7802\nEpoch 354/500\n16/16 - 0s - loss: 0.4558 - accuracy: 0.7798\nEpoch 355/500\n16/16 - 0s - loss: 0.4592 - accuracy: 0.7778\nEpoch 356/500\n16/16 - 0s - loss: 0.4570 - accuracy: 0.7771\nEpoch 357/500\n16/16 - 0s - loss: 0.4567 - accuracy: 0.7787\nEpoch 358/500\n16/16 - 0s - loss: 0.4563 - accuracy: 0.7799\nEpoch 359/500\n16/16 - 0s - loss: 0.4573 - accuracy: 0.7789\nEpoch 360/500\n16/16 - 0s - loss: 0.4570 - accuracy: 0.7800\nEpoch 361/500\n16/16 - 0s - loss: 0.4565 - accuracy: 0.7791\nEpoch 362/500\n16/16 - 0s - loss: 0.4556 - accuracy: 0.7795\nEpoch 363/500\n16/16 - 0s - loss: 0.4565 - accuracy: 0.7789\nEpoch 364/500\n16/16 - 0s - loss: 0.4566 - accuracy: 0.7798\nEpoch 365/500\n16/16 - 0s - loss: 0.4563 - accuracy: 0.7787\nEpoch 366/500\n16/16 - 0s - loss: 0.4555 - accuracy: 0.7799\nEpoch 367/500\n16/16 - 0s - loss: 0.4549 - accuracy: 0.7800\nEpoch 368/500\n16/16 - 0s - loss: 0.4553 - accuracy: 0.7798\nEpoch 369/500\n16/16 - 0s - loss: 0.4560 - accuracy: 0.7791\nEpoch 370/500\n16/16 - 0s - loss: 0.4556 - accuracy: 0.7798\nEpoch 371/500\n16/16 - 0s - loss: 0.4551 - accuracy: 0.7797\nEpoch 372/500\n16/16 - 0s - loss: 0.4581 - accuracy: 0.7780\nEpoch 373/500\n16/16 - 0s - loss: 0.4555 - accuracy: 0.7786\nEpoch 374/500\n16/16 - 0s - loss: 0.4553 - accuracy: 0.7796\nEpoch 375/500\n16/16 - 0s - loss: 0.4552 - accuracy: 0.7790\nEpoch 376/500\n16/16 - 0s - loss: 0.4554 - accuracy: 0.7797\nEpoch 377/500\n16/16 - 0s - loss: 0.4554 - accuracy: 0.7801\nEpoch 378/500\n16/16 - 0s - loss: 0.4556 - accuracy: 0.7800\nEpoch 379/500\n16/16 - 0s - loss: 0.4545 - accuracy: 0.7811\nEpoch 380/500\n16/16 - 0s - loss: 0.4539 - accuracy: 0.7805\nEpoch 381/500\n16/16 - 0s - loss: 0.4555 - accuracy: 0.7785\nEpoch 382/500\n16/16 - 0s - loss: 0.4553 - accuracy: 0.7791\nEpoch 383/500\n16/16 - 0s - loss: 0.4554 - accuracy: 0.7779\nEpoch 384/500\n16/16 - 0s - loss: 0.4540 - accuracy: 0.7816\nEpoch 385/500\n16/16 - 0s - loss: 0.4553 - accuracy: 0.7794\nEpoch 386/500\n16/16 - 0s - loss: 0.4545 - accuracy: 0.7802\nEpoch 387/500\n16/16 - 0s - loss: 0.4541 - accuracy: 0.7802\nEpoch 388/500\n16/16 - 0s - loss: 0.4549 - accuracy: 0.7795\nEpoch 389/500\n16/16 - 0s - loss: 0.4556 - accuracy: 0.7797\nEpoch 390/500\n16/16 - 0s - loss: 0.4539 - accuracy: 0.7805\nEpoch 391/500\n16/16 - 0s - loss: 0.4542 - accuracy: 0.7803\nEpoch 392/500\n16/16 - 0s - loss: 0.4553 - accuracy: 0.7793\nEpoch 393/500\n16/16 - 0s - loss: 0.4542 - accuracy: 0.7793\nEpoch 394/500\n16/16 - 0s - loss: 0.4563 - accuracy: 0.7785\nEpoch 395/500\n16/16 - 0s - loss: 0.4560 - accuracy: 0.7804\nEpoch 396/500\n16/16 - 0s - loss: 0.4550 - accuracy: 0.7806\nEpoch 397/500\n16/16 - 0s - loss: 0.4544 - accuracy: 0.7800\nEpoch 398/500\n16/16 - 0s - loss: 0.4557 - accuracy: 0.7791\nEpoch 399/500\n16/16 - 0s - loss: 0.4531 - accuracy: 0.7798\nEpoch 400/500\n16/16 - 0s - loss: 0.4543 - accuracy: 0.7811\nEpoch 401/500\n16/16 - 0s - loss: 0.4538 - accuracy: 0.7793\nEpoch 402/500\n16/16 - 0s - loss: 0.4526 - accuracy: 0.7807\nEpoch 403/500\n16/16 - 0s - loss: 0.4529 - accuracy: 0.7812\nEpoch 404/500\n16/16 - 0s - loss: 0.4526 - accuracy: 0.7804\nEpoch 405/500\n16/16 - 0s - loss: 0.4525 - accuracy: 0.7815\nEpoch 406/500\n16/16 - 0s - loss: 0.4519 - accuracy: 0.7817\nEpoch 407/500\n16/16 - 0s - loss: 0.4524 - accuracy: 0.7809\nEpoch 408/500\n16/16 - 0s - loss: 0.4517 - accuracy: 0.7812\nEpoch 409/500\n16/16 - 0s - loss: 0.4520 - accuracy: 0.7816\nEpoch 410/500\n16/16 - 0s - loss: 0.4524 - accuracy: 0.7799\nEpoch 411/500\n16/16 - 0s - loss: 0.4519 - accuracy: 0.7805\nEpoch 412/500\n16/16 - 0s - loss: 0.4533 - accuracy: 0.7810\nEpoch 413/500\n16/16 - 0s - loss: 0.4527 - accuracy: 0.7817\nEpoch 414/500\n16/16 - 0s - loss: 0.4530 - accuracy: 0.7799\nEpoch 415/500\n16/16 - 0s - loss: 0.4519 - accuracy: 0.7811\nEpoch 416/500\n16/16 - 0s - loss: 0.4522 - accuracy: 0.7808\nEpoch 417/500\n16/16 - 0s - loss: 0.4527 - accuracy: 0.7817\nEpoch 418/500\n16/16 - 0s - loss: 0.4529 - accuracy: 0.7798\nEpoch 419/500\n16/16 - 0s - loss: 0.4516 - accuracy: 0.7817\nEpoch 420/500\n16/16 - 0s - loss: 0.4513 - accuracy: 0.7828\nEpoch 421/500\n16/16 - 0s - loss: 0.4542 - accuracy: 0.7801\nEpoch 422/500\n16/16 - 0s - loss: 0.4518 - accuracy: 0.7813\nEpoch 423/500\n16/16 - 0s - loss: 0.4518 - accuracy: 0.7809\nEpoch 424/500\n16/16 - 0s - loss: 0.4518 - accuracy: 0.7810\nEpoch 425/500\n16/16 - 0s - loss: 0.4512 - accuracy: 0.7814\nEpoch 426/500\n16/16 - 0s - loss: 0.4510 - accuracy: 0.7832\nEpoch 427/500\n16/16 - 0s - loss: 0.4506 - accuracy: 0.7820\nEpoch 428/500\n16/16 - 0s - loss: 0.4513 - accuracy: 0.7817\nEpoch 429/500\n16/16 - 0s - loss: 0.4515 - accuracy: 0.7824\nEpoch 430/500\n16/16 - 0s - loss: 0.4527 - accuracy: 0.7800\nEpoch 431/500\n16/16 - 0s - loss: 0.4531 - accuracy: 0.7807\nEpoch 432/500\n16/16 - 0s - loss: 0.4520 - accuracy: 0.7816\nEpoch 433/500\n16/16 - 0s - loss: 0.4521 - accuracy: 0.7811\nEpoch 434/500\n16/16 - 0s - loss: 0.4529 - accuracy: 0.7804\nEpoch 435/500\n16/16 - 0s - loss: 0.4539 - accuracy: 0.7784\nEpoch 436/500\n16/16 - 0s - loss: 0.4500 - accuracy: 0.7824\nEpoch 437/500\n16/16 - 0s - loss: 0.4508 - accuracy: 0.7832\nEpoch 438/500\n16/16 - 0s - loss: 0.4500 - accuracy: 0.7835\nEpoch 439/500\n16/16 - 0s - loss: 0.4524 - accuracy: 0.7812\nEpoch 440/500\n16/16 - 0s - loss: 0.4514 - accuracy: 0.7812\nEpoch 441/500\n16/16 - 0s - loss: 0.4510 - accuracy: 0.7828\nEpoch 442/500\n16/16 - 0s - loss: 0.4513 - accuracy: 0.7818\nEpoch 443/500\n16/16 - 0s - loss: 0.4506 - accuracy: 0.7828\nEpoch 444/500\n16/16 - 0s - loss: 0.4534 - accuracy: 0.7800\nEpoch 445/500\n16/16 - 0s - loss: 0.4511 - accuracy: 0.7806\nEpoch 446/500\n16/16 - 0s - loss: 0.4509 - accuracy: 0.7829\nEpoch 447/500\n16/16 - 0s - loss: 0.4512 - accuracy: 0.7828\nEpoch 448/500\n16/16 - 0s - loss: 0.4496 - accuracy: 0.7837\nEpoch 449/500\n16/16 - 0s - loss: 0.4488 - accuracy: 0.7837\nEpoch 450/500\n16/16 - 0s - loss: 0.4498 - accuracy: 0.7827\nEpoch 451/500\n16/16 - 0s - loss: 0.4498 - accuracy: 0.7827\nEpoch 452/500\n16/16 - 0s - loss: 0.4503 - accuracy: 0.7818\nEpoch 453/500\n16/16 - 0s - loss: 0.4492 - accuracy: 0.7840\nEpoch 454/500\n16/16 - 0s - loss: 0.4499 - accuracy: 0.7825\nEpoch 455/500\n16/16 - 0s - loss: 0.4498 - accuracy: 0.7834\nEpoch 456/500\n16/16 - 0s - loss: 0.4507 - accuracy: 0.7815\nEpoch 457/500\n16/16 - 0s - loss: 0.4494 - accuracy: 0.7830\nEpoch 458/500\n16/16 - 0s - loss: 0.4490 - accuracy: 0.7839\nEpoch 459/500\n16/16 - 0s - loss: 0.4489 - accuracy: 0.7827\nEpoch 460/500\n16/16 - 0s - loss: 0.4495 - accuracy: 0.7827\nEpoch 461/500\n16/16 - 0s - loss: 0.4495 - accuracy: 0.7829\nEpoch 462/500\n16/16 - 0s - loss: 0.4490 - accuracy: 0.7825\nEpoch 463/500\n16/16 - 0s - loss: 0.4477 - accuracy: 0.7840\nEpoch 464/500\n16/16 - 0s - loss: 0.4515 - accuracy: 0.7807\nEpoch 465/500\n16/16 - 0s - loss: 0.4521 - accuracy: 0.7831\nEpoch 466/500\n16/16 - 0s - loss: 0.4489 - accuracy: 0.7841\nEpoch 467/500\n16/16 - 0s - loss: 0.4481 - accuracy: 0.7840\nEpoch 468/500\n16/16 - 0s - loss: 0.4482 - accuracy: 0.7840\nEpoch 469/500\n16/16 - 0s - loss: 0.4481 - accuracy: 0.7832\nEpoch 470/500\n16/16 - 0s - loss: 0.4498 - accuracy: 0.7835\nEpoch 471/500\n16/16 - 0s - loss: 0.4484 - accuracy: 0.7836\nEpoch 472/500\n16/16 - 0s - loss: 0.4478 - accuracy: 0.7833\nEpoch 473/500\n16/16 - 0s - loss: 0.4481 - accuracy: 0.7826\nEpoch 474/500\n16/16 - 0s - loss: 0.4492 - accuracy: 0.7833\nEpoch 475/500\n16/16 - 0s - loss: 0.4474 - accuracy: 0.7837\nEpoch 476/500\n16/16 - 0s - loss: 0.4476 - accuracy: 0.7837\nEpoch 477/500\n16/16 - 0s - loss: 0.4477 - accuracy: 0.7846\nEpoch 478/500\n16/16 - 0s - loss: 0.4501 - accuracy: 0.7833\nEpoch 479/500\n16/16 - 0s - loss: 0.4503 - accuracy: 0.7821\nEpoch 480/500\n16/16 - 0s - loss: 0.4490 - accuracy: 0.7836\nEpoch 481/500\n16/16 - 0s - loss: 0.4468 - accuracy: 0.7854\nEpoch 482/500\n16/16 - 0s - loss: 0.4515 - accuracy: 0.7819\nEpoch 483/500\n16/16 - 0s - loss: 0.4479 - accuracy: 0.7831\nEpoch 484/500\n16/16 - 0s - loss: 0.4482 - accuracy: 0.7847\nEpoch 485/500\n16/16 - 0s - loss: 0.4473 - accuracy: 0.7851\nEpoch 486/500\n16/16 - 0s - loss: 0.4476 - accuracy: 0.7827\nEpoch 487/500\n16/16 - 0s - loss: 0.4478 - accuracy: 0.7847\nEpoch 488/500\n16/16 - 0s - loss: 0.4480 - accuracy: 0.7829\nEpoch 489/500\n16/16 - 0s - loss: 0.4499 - accuracy: 0.7824\nEpoch 490/500\n16/16 - 0s - loss: 0.4471 - accuracy: 0.7841\nEpoch 491/500\n16/16 - 0s - loss: 0.4469 - accuracy: 0.7846\nEpoch 492/500\n16/16 - 0s - loss: 0.4461 - accuracy: 0.7842\nEpoch 493/500\n16/16 - 0s - loss: 0.4456 - accuracy: 0.7859\nEpoch 494/500\n16/16 - 0s - loss: 0.4473 - accuracy: 0.7832\nEpoch 495/500\n16/16 - 0s - loss: 0.4463 - accuracy: 0.7859\nEpoch 496/500\n16/16 - 0s - loss: 0.4464 - accuracy: 0.7838\nEpoch 497/500\n16/16 - 0s - loss: 0.4455 - accuracy: 0.7850\nEpoch 498/500\n16/16 - 0s - loss: 0.4460 - accuracy: 0.7844\nEpoch 499/500\n16/16 - 0s - loss: 0.4488 - accuracy: 0.7839\nEpoch 500/500\n16/16 - 0s - loss: 0.4487 - accuracy: 0.7837\n"
    }
   ],
   "source": [
    "#Training the Model\n",
    "import tensorflow as tf\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train_categorical, epochs=500, batch_size=2000, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "964/964 - 3s - loss: 0.4447 - accuracy: 0.7864\n322/322 - 1s - loss: 0.4831 - accuracy: 0.7721\nTRAINING DATA --> Loss: 0.4447202682495117, Accuracy: 0.7864024043083191\nTESTING DATA --> Loss: 0.48307517170906067, Accuracy: 0.7721124887466431\n"
    }
   ],
   "source": [
    "#Evaluate the Model using the testing data\n",
    "#Compare Model performace between training and testing data\n",
    "model_loss_train, model_accuracy_train = model.evaluate(X_train_scaled, y_train_categorical, verbose=2)\n",
    "model_loss, model_accuracy = model.evaluate(X_test_scaled, y_test_categorical, verbose=2)\n",
    "\n",
    "print(f\"TRAINING DATA --> Loss: {model_loss_train}, Accuracy: {model_accuracy_train}\")    \n",
    "print(f\"TESTING DATA --> Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# Define working directory\n",
    "os.chdir(r\"C:\\Users\\hguzm\\Documents\\000. Personal\\Bootcamp\\Proyectos\\FinalProject-Spotify\")\n",
    "# model.save(\"spotify_DeepLearning_Model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model  \n",
    "from tensorflow.keras.models import load_model\n",
    "# Define working directory\n",
    "os.chdir(r\"C:\\Users\\hguzm\\Documents\\000. Personal\\Bootcamp\\Proyectos\\FinalProject-Spotify\")\n",
    "# model = load_model(\"spotify_DeepLearning_Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 14)                168       \n_________________________________________________________________\ndense_1 (Dense)              (None, 120)               1800      \n_________________________________________________________________\ndense_2 (Dense)              (None, 80)                9680      \n_________________________________________________________________\ndense_3 (Dense)              (None, 2)                 162       \n=================================================================\nTotal params: 11,810\nTrainable params: 11,810\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #Evaluate the Model using the testing data\n",
    "# model_loss, model_accuracy = model.evaluate(X_test_scaled, y_test_categorical, verbose=2)\n",
    "    \n",
    "# print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Making Predictions with new data\n",
    "# new_data = np.array([[0.2, 0.3, 0.4,0.2, 0.3, 0.4,0.2, 0.3, 0.4,0.2, 0.3, 0.4,0.2, 0.3, 0.4]])  # AQUI IRIA INFORMACIÓN DEL API\n",
    "# print(f\"Predicted class: {model.predict_classes(new_data)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda621df6d49d2c4311a89d7503a44084b1",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}